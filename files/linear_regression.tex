\section{Linear regression}

In the thesis we use linear regression models in order to model the CLR transformed counts for the different cell types.
This is a summary of what was seen in the linear modeling course by Els Goetghebeur.

\subsection{Simple linear regression}
\subsubsection{Notation}
\begin{itemize}
    \item Capitalized letters (e.g. Y) are random variables. They have a mean and variance.
    \item Small letters (e.g. x, y) are fixed constants or observed values.
    \item Indices are donoted by \textit{i} or \textit{j}.
\end{itemize}

\subsubsection{Meaning and interpretation}
Regression only shows association between the dependent and independent variables.
It is not because two things tend to go in the same direction, that a change in one is \textit{causing} the change in the other.
It is possible that they are both causally responding to a third factor that is changing.
Causal relationships need additional assumptions and/or data to be interpreted.

\subsubsection{Regression model formally}
A regression model describes a statistical relationship between a random variable $Y$ and a fixed x or random $X$.
It describes how the distribution of the random variable $Y$ varies with given values of x, and the association between $Y$ and x.

The simple regression model captures this by a parametric form for the distribution $Y$ for every level of x (e.g. the normal distribution with $\mu_x$ and $\sigma_x^2$), and a functional form for the means of these distributions $\mu_x$, which may depend on unknown parameters (e.g. $\mu_x = \beta_0 + \beta_1 x$).

\subsubsection{The simple linear regression model}

The Simple Linear Regression model takes the following form (Formula \ref{formula:SLR}). 

\begin{equation}
    Y_i = \beta_0 + \beta_1 x_1 + \epsilon_i \ with \ \epsilon_i \ \text{iid} \ N(0, \sigma^2)
    \label{formula:SLR}
\end{equation}

The model is characterized by the following elements:
\begin{itemize}
    \item $E(Y_i|x_i) = \beta_0 + \beta_1 x_i$
    \item $Var(Y_i|x_i) = \sigma^2$
    \item $E(\epsilon_i) = 0$
    \item $Var(\epsilon_i)$ is constant
    \item The observations are independent (no correlation between any $Y_i, Y_j$)
    \item The data is normally distributed
\end{itemize}

\subsubsection{Interpretation of the coefficients}
$\beta_0$ is the intercept.
It shows the intersection with the y-axis and often does not have an interpretation.
It does have an interpretation when x is centered or standardized, as it then signifies the $Y$ for the average x.
$\beta_1$ is the slope of the regression line.
Per unit increase in x, the $Y$ is expected to increase with an amount equal to $\beta_1$.

\subsubsection{Estimation of the regression function}
The regression coefficients are determined using \textit{ordinary least squares}.
This methods finds the optimal values of $\beta_0$ and $\beta_1$ that minimize the q-function (Formula \ref{formula:q_function}).
This q-function is the sum of the squared residuals.

\begin{equation}
    q(\beta_0, \beta_1) = \sum_{i=1}^{n} \left(y_i - (\beta_0 + \beta_1 x_i)\right)^2
    \label{formula:q_function}
\end{equation}

One can try to solve this minimization problem numerically, however it is much feasible in an analytical way.
For this, the normal equations are used (Formula \ref{formula:normal_equations}), which result in point estimates for $\beta_0$ and $\beta_1$.
These normal equations are acquired by taking the derivative of the q-value towards $\beta_0$ and $\beta_1$, respectively.

\begin{equation}
    \begin{array}{ll}
        \left\{
            \begin{array}{ll}
                \sum y_i = n\beta_0 + \beta_1 \sum x_i \\
                \sum x_i y_i = \beta_0 \sum x_i + \beta_1 \sum x_i^2
            \end{array}
        \right. \\ [10pt]

        \left\{
            \begin{array}{ll}
                \beta_0 = \bar{y} - \beta_1 \bar{x} \\
                \beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{Cov(x,y)}{Var(x)}
            \end{array}
        \right.
    \end{array}
    \label{formula:normal_equations}
\end{equation}

\subsubsection{Estimation of the variance}

We know that the expected value of $y_i$ equals $\mu$ and the variance equals $\sigma^2$.
In an ideal world, this is an unbiased estimator, however, we do not know the value of $\mu$ in real life.
For this reason we can calculate the variance by taking the squared difference over the real and predicted value, and then dividing by the number of samples.
This estimator is biased though, as we do need to account for the unknown $\mu$.
By dividing by the number of samples minus 2 (as we use two times $y$ in the formula), we get an unbiased estimator for the variance, which we call the MSE (Formula \ref{formula:mse}).

\begin{equation}
    MSE = \frac{\sum_{i=1}^n (Y_i - \hat{Y}_i)^2}{n - 2} = \frac{\sum_{i=1}^n E_i^2}{n - 2}
    \label{formula:mse}
\end{equation}

\subsubsection{Parameter estimation using MLE}

Maximum Likelihood Estimation can also be used in order to fit the parameters of the linear regression model.
This works by maximizing the probability density function of $Y$ given a model (Formula \ref{formula:mle_linear_regression}), in this under the assumption that $Y$ is normally distributed.
From the density distribution, we see that we have to estimate three parameters: $\beta_0$, $\beta_1$ and $\sigma^2$.
Although, to do this, we need to calculate the loglikelihood, as maximizing a sum is far easier than maximizing a product.
By taking the partial derivatives towards the parameters of interest, we can derive an estimator for them.
As can be seen, for $\beta_0$ and $\beta_1$, we get the same normal equations (Formula \ref{formula:normal_equations}) as seen previously.

\begin{equation}
    \begin{aligned}
        f(y|\theta) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(- \frac{\left[y - (\beta_0 + \beta_1x)\right]^2}{2\sigma^2}\right), \\[10pt]
        \mathcal{L}(\beta_0, \beta_1, \sigma^2) &= \prod_{i=1}^n \left\{ \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2} \left[\frac{Y_i-\beta_0-\beta_1x_i}{\sigma}\right]^2\right)\right\},
    \end{aligned}
    \label{formula:mle_linear_regression}
\end{equation}

\begin{equation}
    \begin{aligned}
    \text{where} \quad 
    \left\{
    \begin{aligned}
        \frac{d \log (\mathcal{L})}{d\beta_0} &= \frac{1}{\sigma^2} \sum\left(Y_i - \beta_0 - \beta_1x_i\right) = 0, \\
        \frac{d \log (\mathcal{L})}{d\beta_1} &= \frac{1}{\sigma^2} \sum x_i \left(Y_i - \beta_0 - \beta_1x_i\right) = 0, \\
        \frac{d \log (\mathcal{L})}{d\sigma^2} &= -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum \left(Y_i - \beta_0 - \beta_1 x_i\right)^2 = 0
    \end{aligned}
    \right. \\[10pt]
    \text{with results:} \quad \hat{\beta}_0 = B_0, \quad \hat{\beta}_1 = B_1, \quad \hat{\sigma}^2 = \frac{\sum(Y_i - \hat{Y}_i)^2}{n}.
    \end{aligned}
\end{equation}
    
\subsection{Inference on the coefficients}
\subsubsection{Confidence intervals}

The confidence interval is the range in which the true value of $\beta_1$ is in, with 95\% certainty.

\begin{equation}
    CI = \left[\beta_1 \pm t(\frac{1 - \alpha}{2}; n-2) S(\beta_1) \right]
\end{equation}

\subsubsection{Two-sided t-test}
In order to test whether the estimate for $\beta_1$ is different from zero.
For this we set up the following hypotheses:

\begin{align}
    H_0&: \beta_1 = 0
    H_1&: \beta_1 \ne 0
\end{align}

In order to assess this, we calculate the t-statistic using the following formula: $t = \frac{\beta_1 - 0}{S(\beta_1)}$.
A p-value can be found by computing the area under the probability density function with $n-2$ degrees of freedom until $\alpha / 2$ and subtracting it from 1.

\subsubsection{Prediction interval}
The prediction interval is the interval in which 95\% of the predictions will end up being.
This is calculated similarly as the confidence interval.

\begin{equation}
    PI = \left[\hat{Y_h} \pm t(\frac{1 - \alpha}{2}; n-2) S(\hat{Y_h}) \right]
\end{equation}

\subsection{Multiple Linear Regression}
\subsubsection{The multiple linear regression model}
When we add more variables to the model, we get a multiple linear regression model.
This can be written as seen in Formula \ref{formula:multiple_linear_regression}.

\begin{equation}
    \begin{aligned}
        Y_i &= \beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n + \epsilon_i
            &= \beta_0 + \sum_{k=1}^n \beta_k x_{ik} + \epsilon_i
    \end{aligned}
    \label{formula:multiple_linear_regression}
\end{equation}

The $\beta$ coefficients can be interpreted as follows (e.g. $\beta_1$): $\beta_1$ is the change in mean response when $x_1$ is increased by a unit and all other variables remain constant.

Often there can exist interaction between variables, which can be added to the model by incorporating an interaction term $\beta_{12} x_1 x_2$. This can be interpreted as the additional effect on $y$, when $x_1$ is held constant and $x_2$ increases with one unit (and vice versa). For categorical variables, this is the effect of a certain combination of categories on $y$.

\subsubsection{Estimation of the regression function}
In order to fit the regression line, we again minimize the q-function (Formula \ref{formula:q_function_multi})

\begin{equation}
    Q = \sum_{i=1}^n \left(Y_i - \beta_0 - \beta_1x_{i1} - \cdots - \beta_{p-1}x_{i, p-1}\right)^2
    \label{formula:q_function_multi}
\end{equation}

To do this, we solve the least squares normal equations, that take the form of $X'XB = X'Y$, with estimators $B = (X'X)^{-1} (X'Y)$

\begin{equation}
    \begin{aligned}
    \left\{
    \begin{aligned}
        \frac{dQ}{d\beta_0} = \sum_{i=1}^{n} (y_i + \beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i, p-1}) = 0 \\
        \frac{dQ}{d\beta_1} = \sum_{i=1}^{n} (y_i + \beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i, p-1})x_{i1} = 0 \\
        \cdots\\
        \frac{dQ}{d\beta_1} = \sum_{i=1}^{n} (y_i + \beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i, p-1})x_{i, p-1} = 0
    \end{aligned}
    \right.
    \end{aligned}
\end{equation}

\subsection{Weighted least squares}

In weighted least squares, the idea is that weights can be used to deal with heteroscedastic data.
Weights are user defined (e.g. the inverse of the variance of the variables).
The weighted least squares estimator is defined as in Formula \ref{formula:WLS}.
Deriving which weights to use is often the hardest part.

\begin{equation}
    \begin{aligned}
        \hat{\beta}_{WLS}   &= \arg\min_{\beta} \sum_{i=1}^{n} \epsilon_i^{*2} 
                            &= (X^TWX)^{-1} X^TWY
    \end{aligned}
    \label{formula:WLS}
\end{equation}

